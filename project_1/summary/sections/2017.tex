% \section{2017 - Suchitra Chinnamail, Phuc Dang}
    \section{FootballWhispers: Transfer Rumour Detection}

    % \url{https://iswc2017.ai.wu.ac.at/wp-content/uploads/papers/IndustryTrack/ISWC2017_paper_495.pdf}
    % \bigskip 
    
    Football whispers  provides a service of showcasing the rumours of  transfers of players between teams. The problem that needs to be addressed to showcase the rumours, is the estimation of the veracity of the rumours, by providing relative likelihood of a player moving into a specific team.
    The likelihood is predicted using the Twitter conversations related to transfers. This process has two processes, Named-Entity Linking (NEL); and the assessment and combination of rumors to determine their relative likelihood. 
    As rumor detection and the use of knowledge graph data was evolving, this led to significantly increased player NEL performance, and identifying of these rumors concerned actual 2016 football transfers.The primary reason for this is the availability of multilingual data in the KGs and the use of language agnostic statistical disambiguation techniques in NEL.
 

    \section{Semantic Concept Discovery Over Event Data}

    % \url{https://iswc2017.semanticweb.org/paper-478/}
    % \bigskip 
    Preparing a report on a topic or a question which is comprehensive, accurate and unbiased is a challenge by itself. The first step as a part of the process is daunting discovery task which requires searching a number of information sources without introducing bias from analyst’s current knowledge or limitations of these information sources. For most of the analysis reports a common requirement is a deep understanding of different kinds of historical and ongoing reported events in the media.
    The goal is to provide a unified solution allowing deeper understanding of the same data which is used to perform other analysis tasks.This is done by creating a framework to discover event databases using semantic web technologies.
    The system takes a set of event databases and RDF knowledge bases as input and provides a set of APIs as output. These APIs provide a unified retrieval mechanism on input data and knowledge bases, and an interface to various concept discovery algorithms.

    \section{Federated Semantic Data Management for Business Intelligence and Healthcare: Two Case Studies}

    % \url{https://iswc2017.ai.wu.ac.at/wp-content/uploads/papers/IndustryTrack/ISWC2017_paper_499.pdf}
    % \bigskip 
    In a large e-commerce company, the inconsistency between departments like IT, fulfillment, and accountant departments when they process data. In addition, in a hospital, the system misses to identify patients who need mechanical device (Left Ventricular Assist Device – LVAD)
    Using the structure of Ontology Based Data Access (OBDA): OWL (Web Ontology Language) as uniform conceptual model describing the interest and stored data; R2RML (RDB to RDF Mapping Language) to map ontology with the source database; SPARQL is an RDF Query Language for managing the heterogeneous source database.
    When applying the structure to real-world situations, an e-commerce company shows that the time and cost have been reduced to generate reports. Meanwhile, the hospital shows that more patients are identified.

    \section{The Qualification Data Repository (QDR): Enhancing Interoperability of Qualifications in Europe}

    % \url{https://iswc2017.ai.wu.ac.at/wp-content/uploads/papers/IndustryTrack/ISWC2017_paper_503.pdf}
    % \bigskip 
    
    In the area of employment across Europe, the stakeholders (jobseekers, learners, education institutes, employment services, Member States authorities, etc) need to share information to support recruitment and career management.
    QDR as a software component allows providers of data to upload datasets on European web portals, online service, and in semantic assets. During the operation, QMS (Qualification Metadata Schema) ensures consistency and guarantees the translation of dataset.
    Developing QDR is not an easy task. 2 problems: size of dataset is big, like versioning and validation. Secondly, the inconvenience of working with RDF directly from the front end.


